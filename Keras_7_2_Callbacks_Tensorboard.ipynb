{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks\n",
    "* Model Checkpointing\n",
    "* Early Stopping - interrupting training when the validation loss is no longer improving\n",
    "* Dynamically adjusting the value of certain parameters during training - such as learning rate of the optimizer\n",
    "* Logging training and validation metrics during training, or visualizing the representation learned by the model as they're update\n",
    "\n",
    "keras.callbacks includes:\n",
    "keras.callbacks.ModelCheckpoint\n",
    "keras.callbacks.EarlyStopping\n",
    "keras.callbacks.ReduceLROnPlateau\n",
    "keras.callbacks.CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1,),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='my_model.h5', monitor='val_loss', save_best_only=True,)\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(x, y, epochs=10, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceLROnPlauteau callback\n",
    "Reduce the learning rate when validation loss has stopped improving. Reducing or increasing the learning rate in the case of a loss pleateau is an effective strategy to get out of local minima during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback will reduce learning rate by 10 when triggered after the validation loss has stopped improving for 10 epochs\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,)\n",
    "]\n",
    "model.fit(x, y, epochs=10, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "Browser based visualization tools that comes packaged with TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 2000 #Number of words to consider as features\n",
    "max_len = 500 #Cuts off text after this number of words\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len, name='embed'))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.4599 - acc: 0.8104 - val_loss: 0.4767 - val_acc: 0.8068\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.4082 - acc: 0.7946 - val_loss: 0.4580 - val_acc: 0.7922c: 0. - ETA: 0s - loss: 0.41\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.3558 - acc: 0.7705 - val_loss: 0.6046 - val_acc: 0.7282- loss: 0.3441 \n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 3s 169us/step - loss: 0.2925 - acc: 0.7597 - val_loss: 0.5492 - val_acc: 0.7060\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 0.2842 - acc: 0.6946 - val_loss: 0.6352 - val_acc: 0.6442\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.2262 - acc: 0.6556 - val_loss: 0.7341 - val_acc: 0.5520\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.1864 - acc: 0.5951 - val_loss: 0.7997 - val_acc: 0.5036\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 0.1556 - acc: 0.5266 - val_loss: 0.9038 - val_acc: 0.4446ss: 0.139 - ETA: 1s - loss: 0.1\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 0.1297 - acc: 0.4479 - val_loss: 1.0581 - val_acc: 0.3764\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 0.1090 - acc: 0.4043 - val_loss: 1.0352 - val_acc: 0.3668\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 3s 163us/step - loss: 0.1018 - acc: 0.3414 - val_loss: 1.0131 - val_acc: 0.3300\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 0.1008 - acc: 0.2974 - val_loss: 1.1743 - val_acc: 0.3034\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 3s 164us/step - loss: 0.0950 - acc: 0.2495 - val_loss: 1.2738 - val_acc: 0.2720\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 3s 167us/step - loss: 0.0970 - acc: 0.2166 - val_loss: 1.1801 - val_acc: 0.2642\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 3s 165us/step - loss: 0.1009 - acc: 0.1862 - val_loss: 1.1753 - val_acc: 0.2534\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 0.0931 - acc: 0.1695 - val_loss: 1.2483 - val_acc: 0.2456\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.0942 - acc: 0.1591 - val_loss: 1.3645 - val_acc: 0.2154\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 3s 163us/step - loss: 0.0921 - acc: 0.1374 - val_loss: 1.2598 - val_acc: 0.2272\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 3s 166us/step - loss: 0.0991 - acc: 0.1230 - val_loss: 1.3063 - val_acc: 0.2040\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.0954 - acc: 0.1166 - val_loss: 1.3705 - val_acc: 0.2148\n"
     ]
    }
   ],
   "source": [
    "# records activation histographs AND embedding data every 1 epoch\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='my_log_dir', histogram_freq=1)\n",
    "]\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
