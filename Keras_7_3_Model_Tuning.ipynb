{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 'works okay' to 'works great and wins ML competitions'\n",
    "* Residual connections\n",
    "* Normalization\n",
    "* Depth-wise seperable convolution\n",
    "\n",
    "##### Batch Normalization\n",
    "* normalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)\n",
    "* BATCH normalization is a type of layer (BatchNormalization in Keras)\n",
    "* Batch normalization works by internally maintaining an eponential moving average of the batch-wise mean and variance seen during training. The main effect is to help with gradient propogation\n",
    "* Used extensively in advanced covnet architectures in Keras such as ResNet50, Inception V3, and Xception\n",
    "* Typically used after a convolutional or densely connected layer\n",
    "\n",
    "default axis = -1 (Dense, Conv1D, RNN, and Conv2D with data_format set to \"channels_last\")\n",
    "features_axis = 1 (Conv2D with data_format set to \"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "conv_model = keras.models.Sequential() \n",
    "conv_model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "conv_model.add(layers.BatchNormalization())\n",
    "\n",
    "dense_model = keras.models.Sequential()\n",
    "dense_model.add(layers.Dense(32, activation='relu'))\n",
    "dense_model.add(layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise Seperable Convolution\n",
    "* Lighter (fewer trainable parameters)\n",
    "* Faster (fewer floating point operations)\n",
    "* Few % points BETTER on it's task\n",
    "# SeperableConv2D\n",
    "\n",
    "Advantages especially importnat when you're training small models from scratch on limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pgbpr\\Anaconda2\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "num_class = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.SeparableConv2D(32, 3, activation='relu', input_shape=(height, width, channels,)))\n",
    "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(num_class, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "Arbritary Decisions\n",
    "* How many layers to stack\n",
    "* How many units or filters in each layer\n",
    "* Should you use 'relu' - or a different activation function\n",
    "* Should you use BatchNormalization after a given layer\n",
    "* How much dropout should you use\n",
    "* ... And so on\n",
    "\n",
    "Often turns out that random search (choosing hyperparams to evaluate at random) is the best solution.\n",
    "But Hyperopt (https://github.com/hyperopt/hyperopt)\n",
    "Hyperopt keras implementation (https://github.com/maxpumperla/hyperas) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensembling\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "...\n",
    "preds_z = model_z.predict(x_val)\n",
    "\n",
    "final_preds = 0.5*preds_a + 0.1*preds_b +....\n",
    "Where 0.5, 0.1, etc are assumed to be learned empirically\n",
    "\n",
    "Search for good ensembling weights: random search or simple optimization such as Nelder-Mead.\n",
    "\n",
    "You should ensemble models that are as good as possible whilst being as different as possible.\n",
    "\n",
    "Example: Ensemble gradient boosted decision trees and neural network based models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
