{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced RNN\n",
    "* Recurrent Dropout: Dropout for overfitting RNNs\n",
    "* Stacking recurrent layers: Increasing representational power of RNN\n",
    "* Bidirectional recurrent layers: Present same information to RNN in different ways, increasing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = r'C:\\Users\\pgbpr\\Documents\\Verusen\\jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "temp = float_data[:, 1] # temperature (in degrees celsius)\n",
    "plt.plot(range(len(temp)),  temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1440), temp[:1440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data with stupid naive unclear implementation\n",
    "\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "* Data: The original array of floating point data, which is normalized\n",
    "* Lookback: how many timesteps back the input data should go\n",
    "* delay: How many timesteps in the future the target should be\n",
    "* min_index and max_index: Indices in the data array that delimit which timesteps to draw from. \n",
    "* shuffle: whether to shuffle the samples or draw them in chronological order\n",
    "* step: the period, in timesteps, at which you sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "\n",
    "    ## Shift the starting index\n",
    "    nbatch = (max_index - min_index - lookback) // batch_size\n",
    "    shift = max_index - min_index - lookback - nbatch*batch_size\n",
    "    min_index_trunc = min_index + shift + lookback - 1\n",
    "\n",
    "    i = min_index_trunc\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index_trunc, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index_trunc\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "train_gen = generator(\n",
    "    float_data, \n",
    "    lookback=lookback, \n",
    "    delay=delay, \n",
    "    min_index=0, \n",
    "    max_index=200000, \n",
    "    shuffle=True, \n",
    "    step=step, \n",
    "    batch_size=batch_size)\n",
    "val_gen = generator(\n",
    "    float_data, \n",
    "    lookback=lookback, \n",
    "    delay=delay, \n",
    "    min_index=200001, \n",
    "    max_index=300000, \n",
    "    step=step, \n",
    "    batch_size=batch_size)\n",
    "test_gen = generator(\n",
    "    float_data, \n",
    "    lookback=lookback, \n",
    "    delay=delay, \n",
    "    min_index=300001, \n",
    "    max_index=None, \n",
    "    step=step, \n",
    "    batch_size=batch_size)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n",
    "\n",
    "val_steps = 10000\n",
    "test_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    i = 0\n",
    "    for step in range(val_steps):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "evaluate_naive_method() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_mae = 0.29 * std[1]\n",
    "celsius_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# model.compile(optimizer=RMSprop(), loss='mae')\n",
    "# history = model.fit_generator(train_gen, steps_per_epoch=500, epochs=20, validation_data=val_gen, \n",
    "#                                    validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                              callbacks = callbacks,\n",
    "                              epochs=20, validation_data=val_gen, \n",
    "                              validation_steps=val_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout and recurrent dropout to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized GRU\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                              callbacks = callbacks,\n",
    "                              epochs=5\n",
    ", validation_data=val_gen, \n",
    "                              validation_steps=val_steps)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once overfitting has been dealt with, try adding another recurrent layer\n",
    "Recurrent layer stacking is a classic way to build more-powerful RNNs.\n",
    "Google translate is a stack of 7 large LSTM layers (huge)\n",
    "\n",
    "To stack recurrent layers:\n",
    "Must return a full 3D tensor (return_sequences = True)\n",
    "And you must apparently add an activation to the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized GRU, with additional layer\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=500,\n",
    "                              callbacks = callbacks,\n",
    "                              epochs=20, validation_data=val_gen, \n",
    "                              validation_steps=val_steps)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "Used FREQUENTLY in NLP! {So it's super important to me}\n",
    "RNNs are order dependent. Shuffling / reversing the timesteps can completely change the representations learned by the RNN.\n",
    "\n",
    "A bidirectional rnn: two regular RNNs (GRU or LSTM). One processes the input sequence in one direction, and another processes it in the opposite direction.\n",
    "\n",
    "Bidirectional RNNs can catch patterns that may be overlooked by unidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All you need to do is...\n",
    "def generator:\n",
    "    ...\n",
    "    ...\n",
    "    yield samples[:, ::-1, :], targets\n",
    "    \n",
    "### However - this will UNDERPERFORM the previous method. The underlying GRU layer will typically be better at\n",
    "### remembering the RECENT PAST than the DISTANT past. More recent data points are more predictive than older\n",
    "### data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed order RNN (IMDB example)\n",
    "\n",
    "This is missing dropout and recurrent_dropout in the first LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 173s 9ms/step - loss: 0.4851 - acc: 0.7702 - val_loss: 0.3512 - val_acc: 0.8610\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 170s 8ms/step - loss: 0.3088 - acc: 0.8818 - val_loss: 0.4249 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 169s 8ms/step - loss: 0.2578 - acc: 0.9041 - val_loss: 0.3206 - val_acc: 0.8638\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 165s 8ms/step - loss: 0.2145 - acc: 0.9217 - val_loss: 0.3721 - val_acc: 0.8794\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 164s 8ms/step - loss: 0.1927 - acc: 0.9333 - val_loss: 0.3522 - val_acc: 0.8744\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 163s 8ms/step - loss: 0.1720 - acc: 0.9392 - val_loss: 0.6329 - val_acc: 0.8066\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 164s 8ms/step - loss: 0.1547 - acc: 0.9466 - val_loss: 0.4449 - val_acc: 0.8720\n",
      "Epoch 8/10\n",
      " 3072/20000 [===>..........................] - ETA: 2:14 - loss: 0.1139 - acc: 0.9583"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='Sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "Adjust the # of units in each recurrent layer in the stacked setup.\n",
    "Adjust the learning rate use by the RMSprop optimizer\n",
    "Try using LSTM layers instead of GRU layers\n",
    "Try using a bigger densely connected regressor on top of the recurrent layers: bigger Dense Layer or stack of Dense Layers\n",
    "Run the best-performing models against the test set to ensure no overfitting to validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "* First establish common sense baseline. Dummy Regressors\n",
    "* Try simple models before expensive ones\n",
    "* If temporal order matters, use RNNs\n",
    "* Add dropout to RNNs by using a time-constant dropout mas and a recurrent dropout mask (dropout, recurrent_dropout)\n",
    "* Stacked RNNs provide more representational power, much more expensive, not always worth it. \n",
    "* Bidirectional RNNs are useful for natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important - Recurrent Attention and Sequence Masking (NLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
