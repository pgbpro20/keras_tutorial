{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation for Neural Networks\n",
    "Data stored in multidimensional Numpy Arrays ==> Tensors\n",
    "Number of dimensions is often called an _axis_\n",
    "\n",
    "*Terms*\n",
    "* Rank / Number of Axes: also called tensors ndim. This is what you normally think of dimensionality in arrays\n",
    "* Shape: Tuple of integers that describes how many dimensions the tensor has along each axis e.g. matrix (3, 5) or 3D tensor of the shape (3, 3, 5)\n",
    "* Data Type: type of data contained in the tensor. Float32, uint8, Float64, occassionally char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalers 0-D Tensors\n",
    "# float32 or float64\n",
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(x)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12  3  6 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectors (1-D tensors)\n",
    "# This is a 5-DIMENSIONAL VECTOR\n",
    "x = np.array([12, 3, 6, 14])\n",
    "print(x)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 78  2 34  0]\n",
      " [ 6 79  3 35  1]\n",
      " [ 7 80  4 36  2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrices (2D tensors)\n",
    "# 2 axis - rows and columns\n",
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "print(x)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+ Dimensional Tensors\n",
    "#x = np.array([[[5, 78, 2, 34, 0],\n",
    "#               [6, 79, 3, 35, 1],\n",
    "#               [7, 80, 4, 36, 2],\n",
    "#              [[5, 78, 2, 34, 0].....]]])\n",
    "# Basically it's just nesting arrays within arrays. It's a little awkwardly visualized in the book and above \n",
    "# and with numpy in general I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) =  mnist.load_data()\n",
    "print(train_images.ndim)\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)\n",
    "digit = train_images[4]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images overall shape is (60000, 28, 28)\n",
      "Now three equivalents ways to slice images 10-100\n",
      "(90, 28, 28)\n",
      "(90, 28, 28)\n",
      "(90, 28, 28)\n",
      "More examples like train_images[:, 14:, 14:]\n",
      "(60000, 14, 14)\n",
      "Negative indices - as you know\n",
      "(60000, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "# 2.2.6: Manipulating Tensors\n",
    "# Slices images 10-100 (100 not included)\n",
    "print(f\"Train images overall shape is {train_images.shape}\")\n",
    "print(\"Now three equivalents ways to slice images 10-100\")\n",
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)\n",
    "print(train_images[10:100, :, :].shape)\n",
    "print(train_images[10:100, 0:28, 0:28].shape)\n",
    "\n",
    "print('More examples like train_images[:, 14:, 14:]')\n",
    "print(train_images[:, 14:, 14:].shape)\n",
    "\n",
    "print('Negative indices - as you know')\n",
    "print(train_images[:, 7:-7, 7:-7].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batches\n",
    "First axis - Axis 0 - will be the samples axis- In the MNIST: the samples are digits\n",
    "AKA _batch axis_ or _batch dimension_\n",
    "\n",
    "Deep learning models don't process an entire dataset at once (full gradient descent)\n",
    "Instead they process data in small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[:128]\n",
    "batch = train_images[128:256]\n",
    "n=2\n",
    "batch = train_images[128 * n:128 * (n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Data Tensors\n",
    "Vector Data: 2D tensors of shape (samples, features) \n",
    "Samples X Features\n",
    "Each single data point can be encoded as a vectore, batch of data will be encoded as a 2D tensor (array of vectors). 1st axis is samples axis, 2nd axis is features axis\n",
    "* Ex1: Each person can be charcterized as a vector of 3 values: age, zip code, income. There are 100,000 people. This can be stored as a 2D tensor of shape (100000, 3)\n",
    "* Ex2: Dataset of text docs, each doc represented by the counds of how many times each word appears out of a dictionary of 20,000 common words. Each doc encoded as a vector of 20,000 values (one count per word in the dict). The entire dataset of 500 documents can be stored as a tensor of shape (500, 20000)\n",
    "\n",
    "\n",
    "Timeseries data or sequence data: 3D tensors of shape (samples, timesteps, features)\n",
    "Samples X Timesteps X Features. Time axis is ALWAYS THE SECOND AXIS.\n",
    "* Ex1: Dataset of stock prices. Every minute, stored the current price of the stock. The highest price in the last minute, the lowest price in the last minute. Every day is encoded as a 2D tensor of shape (390, 3) where 390 is the minutes in a trading day. 250 days would be encoded into a 3d tensor (250, 390, 3).\n",
    "* Ex2: Tweets. 280 char limit, Each char encoded as a binary vect of size 128 (ONE HOT ENCODED chars). Each tweet encoded as a 2D tensor (280, 128). 1M tweets encoded to (1M, 280, 128)\n",
    "\n",
    "\n",
    "Images: 4D tensors of shape (samples, height, width, channels) OR (samples, channels, height, width)\n",
    "* Ex1: Batch of greyscale images: 128 greyscale images of size 256 x 256 stored into (128, 256, 256, 1) where the 1 is the single color channel. \n",
    "* Ex2: 128 color 256x256 images could be stored in a tensor of shape (128, 256, 256, 3). Where 3 is the RGB channel (I assume).\n",
    "CONVENTION: Channel-last (tensorflow) - (samples, height, width, color_depth). Channel-first (theano) (samples, color_depth, height, width).\n",
    "\n",
    "Video: 5D tensors of shape (samples, frames, height, width, channels) OR (samples, frames, channels, height, width)\n",
    "* Ex1: 60-second, 144 x 256 YouTube clip sampled at 4 fps would have 240 frames. (4, 240, 144, 256, 3) Total of 105,168,320 values! In float32 = 405 mB. Real videos are compressed by large factor (e.x. MPEG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations\n",
    "keras.layers.Dense(512, activation='relu') \n",
    "W is a 2-D tensor, and b is a vector - both layer attributes\n",
    "output = relu(dot(W, input) + b)\n",
    "W DOT INPUT --> Dot Product\n",
    "+ B --> Matrix Addition\n",
    "relu(x) --> max (x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Python Implentation in a for loop\n",
    "# Relu activation function \n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2 #x is a 2D numpy tensor\n",
    "    x = x.copy() #don't overwrite input tensor\n",
    "    for i in range(x.shape[0]): #for each sample\n",
    "        for j in range(x.shape[1]): #for each feature\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "# element wise addition\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    library_dirs = ['C:\\\\projects\\\\numpy-wheels\\\\numpy\\\\build\\\\openblas']\n",
      "    libraries = ['openblas']\n",
      "    language = f77\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n"
     ]
    }
   ],
   "source": [
    "# Real world - use BLAS: Basic Linear Algebra Subprograms\n",
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have BLAS \n",
    "But if it turns out I absolutely need it\n",
    "http://scipy.github.io/devdocs/building/windows.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real world numpy implementation\n",
    "# import numpy as np\n",
    "# z = x + y\n",
    "# z = np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "Consider X (32, 10), & Y (10 ,)\n",
    "\n",
    "Procedure:\n",
    "1. Add empty first axis to Y (1, 10)\n",
    "2. Repeat Y 32 times (32, 10). Ending up with Y[i, :] = Y for i in range(0, 32)\n",
    "3. Now you can add X + Y\n",
    "\n",
    "Generally apply if one tensor has shape (a, b, ...n, n+1, ...m) and other tensor has shape (n, n+1, ... m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive implementation\n",
    "def naive_add_matrix_and_vector(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3, 32, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "z = np.maximum(x, y) #this will BROADCAST y to the shape of x\n",
    "\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dot\n",
    "\n",
    "```import numpy as np\n",
    "z = np.dot(x, y) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "\n",
    "### or better yet!\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot([x[i, :], y])\n",
    "    return z\n",
    "\n",
    "### now for arbritary number of axes\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0], y.shape[1])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOT PRODUCT SHAPE COMPATIBILITY\n",
    "x.shape: (a, b)\n",
    "\n",
    "y.shape: (b, c)\n",
    "\n",
    "(a, b, c, d) DOT (d,) --> (a, b, c)\n",
    "(a, b, c, d) DOT (d, e) --> (a, b, c, e)\n",
    "(a, b, c, d) DOT (d, e, f) --> (a, b, c, f) ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(6, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Tensor Reshaping\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "x = np.array([[0., 1.],\n",
    "            [2., 3.],\n",
    "            [4., 5.]])\n",
    "print(x.shape)\n",
    "x = x.reshape((6, 1))\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "# Transpose - flip rows and columns duh\n",
    "x = np.zeros((300, 20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "Mental image: optimization process as a small ball rolling down the loss curve. If it has enough momentum, the ball won't get stuck in a ravine and will end up at the global minimum. Momentum is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (result of past acceleration). In practice this means updating the parameter w based not only on the current gradient value but also the previous parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_momentum:\n",
    "    past_velocity = 0.\n",
    "    momentum = 0.1\n",
    "    while loss > 0.01:\n",
    "        w, loss, gradient = get_current_parameters()\n",
    "        velocity = past_velocity * momentum + learning_rate * gradient\n",
    "        w = w + momentum * velocity - learning_rate * gradient\n",
    "        past_velocity = velocity\n",
    "        update_parameter(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation \n",
    "AKA reverse-mode differentiation\n",
    "Backpropogation starts with final loss value and works backward from thetop layers to the bottom layers, applying the chain rule, to compute the contribution that each parameter had in the loss value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking again at MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28, )))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0729 - val_acc: 0.9838\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0773 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0804 - val_acc: 0.9818\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0932 - val_acc: 0.9805\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0880 - val_acc: 0.9821\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0956 - val_acc: 0.9811\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0958 - val_acc: 0.9820\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0923 - val_acc: 0.9819\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0982 - val_acc: 0.9820\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 8.9753e-04 - acc: 0.9998 - val_loss: 0.1043 - val_acc: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1f9ecf6d8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
